# 시스템 로깅 개선하기: 여러 크롤러 로그의 중앙화·구조화

코딧에서는 여러 개의 크롤러를 운영하며, 각 크롤러가 독립된 디렉터리에 매일 회전 로그를 기록합니다.  
기존 방식은 간단하지만, 로그를 한눈에 모아보기 어렵고 “어떤 크롤러가 멈췄는지” 파악하기가 번거롭다는 치명적 단점이 있었습니다.

---

## 목차

1. [기존 로깅 방식](#%EA%B8%B0%EC%A1%B4-%EB%A1%9C%EA%B9%85-%EB%B0%A9%EC%8B%9D)
    
2. [문제점](#%EB%AC%B8%EC%A0%9C%EC%A0%90)
    
3. [개선 목표](#%EA%B0%9C%EC%84%A0-%EB%AA%A9%ED%91%9C)
    
4. [개선된 로깅 구조](#%EA%B0%9C%EC%84%A0%EB%90%9C-%EB%A1%9C%EA%B9%85-%EA%B5%AC%EC%A1%B0)
    
5. [구현 예시 코드](#%EA%B5%AC%ED%98%84-%EC%98%88%EC%8B%9C-%EC%BD%94%EB%93%9C)
    
6. [운영 시나리오 & 모니터링](#%EC%9A%B4%EC%98%81-%EC%8B%9C%EB%82%98%EB%A6%AC%EC%98%A4--%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81)
    
7. [결론](#%EA%B2%B0%EB%A1%A0)
    

---

## 기존 로깅 방식

```javascript
// logger.js
module.exports.getLogger = (origin, filename = '') => {
  let name = filename;
  try {
    name = name.split(/(\\|\/)/g).pop();
  } catch (err) {
    winston.error(err);
  }

  return winston.createLogger({
    level: 'info',
    format: winston.format.combine(
      winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
      winston.format.printf(info => {
        return `[${info.timestamp}][${info.level}][${name}] ${info.message}`;
      })
    ),
    transports: [
      new WinstonDaily({
        level: 'info',
        datePattern: 'YYYY-MM-DD',
        dirname: origin,
        filename: 'info-%DATE%.log',
        maxSize: '500mb',
        maxFiles: '30d',
        zippedArchive: true
      }),
      new WinstonDaily({
        level: 'error',
        datePattern: 'YYYY-MM-DD',
        dirname: origin,
        filename: 'error-%DATE%.log',
        maxSize: '500mb',
        maxFiles: '30d',
        zippedArchive: true
      })
    ]
  });
};

```

- 각 크롤러마다 `origin` 디렉터리를 달리 지정
    
- `info`/`error` 로그를 분리해 파일로 남김
    

---

## 문제점

1. **로그 흩어짐**

    - 크롤러별 디렉터리가 달라 로깅 위치를 일일이 확인해야 함
        
2. **검색·집계 불편**
    
    - 전체 에러 추세나 특정 크롤러 장애를 파악하려면 스크립트나 수동 검색이 필수
        
3. **메타데이터 부족**
    
    - “어느 크롤러”가 남긴 로그인지 명시적 구분이 어려움
        

---

## 개선 목표

- **중앙집중식 저장**: 모든 크롤러 로그를 한 곳(예: Elasticsearch)으로 전송
    
- **구조화된 JSON 포맷**: 필드 기반 검색·필터링 지원
    
- **메타데이터 추가**: `crawlerName`, `environment`, `hostname` 등을 기본 메타 정보로 포함
    
- **모니터링 연동**: Kibana·Grafana 대시보드에서 실시간 장애 감지
    

---

## 개선된 로깅 구조

1. **공통 `logger.js` 모듈화**
    
2. **Winston ‘Elasticsearch’ 전송 추가**
    
3. **DailyRotateFile + Console + ES Transport** 조합
    
4. **JSON 형식 + defaultMeta** 활용
    

---

## 구현 예시 코드

```javascript
// logger.js
const path = require('path');
const winston = require('winston');
require('winston-daily-rotate-file');
const { ElasticsearchTransport } = require('winston-elasticsearch');

const esTransportOpts = {
  level: 'info',
  clientOpts: { node: process.env.ES_NODE_URL },
  indexPrefix: 'crawler-logs',
};

const esTransport = new ElasticsearchTransport(esTransportOpts);

function createLogger(crawlerName) {
  return winston.createLogger({
    level: process.env.NODE_ENV === 'production' ? 'info' : 'debug',
    defaultMeta: {
      service: 'codit-crawler',
      crawler: crawlerName,
      hostname: require('os').hostname()
    },
    format: winston.format.combine(
      winston.format.timestamp(),
      winston.format.json()
    ),
    transports: [
      new winston.transports.DailyRotateFile({
        level: 'info',
        dirname: path.join(process.env.LOG_DIR, 'info'),
        filename: `${crawlerName}-%DATE%.log`,
        datePattern: 'YYYY-MM-DD',
        maxSize: '500m',
        maxFiles: '30d'
      }),
      new winston.transports.DailyRotateFile({
        level: 'error',
        dirname: path.join(process.env.LOG_DIR, 'error'),
        filename: `${crawlerName}-%DATE%.error.log`,
        datePattern: 'YYYY-MM-DD',
        maxSize: '500m',
        maxFiles: '30d'
      }),
      new winston.transports.Console(),
      esTransport
    ]
  });
}

module.exports = { createLogger };
```

- **`defaultMeta`** 로 모든 로그에 `crawler` 이름 삽입
    
- **JSON 포맷** 으로 Kibana에서 바로 분석·필터링 가능
    
- **ES Transport** 로 중앙화 저장 → 대시보드 활용
    

---

## 운영 시나리오 & 모니터링

1. **로그 수집기**
    - Filebeat or Logstash를 통해 ES로 인덱싱
        
2. **Kibana 대시보드**
    - 크롤러별 에러 건수, 응답 시간 분포, 최근 heartbeat 상태 표시
        
3. **알람 설정**
    - 특정 크롤러의 에러율이 5% 초과 시 Slack 알림
        
4. **헬스체크 로그**
```javascript
// 각 크롤러 진입 시
const logger = createLogger('news-crawler');
setInterval(() => {
  logger.info({ event: 'heartbeat' });
}, 5 * 60 * 1000);
```

    

---

## 결론

- **가시성 확보**: 중앙화된 JSON 로그로 장애 지점 즉시 파악
- **유지보수 효율화**: 코드 한 곳만 수정해도 모든 크롤러에 동일 적용
- **확장성**: 새로운 크롤러 추가 시 `createLogger('new-crawler')` 호출만으로 통합
- **모니터링 연계**: 대시보드·알람 시스템과 유연하게 연결 가능

위 구조를 통해 코딧의 크롤러 운영 안정성과 개발 생산성을 한층 끌어올릴 수 있었습니다.